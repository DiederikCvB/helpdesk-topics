{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157920aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b70068",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96193bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/ciphix.csv', header=None, names=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5841f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c530446",
   "metadata": {},
   "source": [
    "# Look around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b8bd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b63200",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"wordcount\"] = df[\"text\"].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0db730",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wordcount'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e390f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aaec8e",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Looking around I noticed:\n",
    "- all messages start with some kind of username handle\n",
    "- some have multiple username handles\n",
    "- some end with a different tag ^ followed by user acronym\n",
    "- different languages\n",
    "- smileys\n",
    "- URLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4975de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check the counts of the first tag mentioned\n",
    "split = df['text'].str.extract(r'(@([a-zA-Z\\d]+)([^\\S\\r\\n]))(.*)')\n",
    "split['text'] = split[3]\n",
    "split['tag'] = split[1]\n",
    "split = split[['tag','text']]\n",
    "split = split.dropna(subset='text')\n",
    "split['tag'].value_counts()[:20].plot(kind='barh', figsize=(10, 8))\n",
    "plt.title(\"Counts of tag first-mentioned\", y=1.02);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc657ead",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f054cf3",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all @ tags\n",
    "def remove_ats(text):\n",
    "    print(text)\n",
    "    at_pattern = re.compile('@[a-zA-Z\\d_]+')\n",
    "    return at_pattern.sub(r'', text)\n",
    "\n",
    "#Remove all employee tags\n",
    "#Tags occur at the end of the line with capital letters and prefix '-' or '^'\n",
    "def remove_tag(text):\n",
    "    at_pattern = re.compile('[\\^\\-][A-Z\\d]+$')\n",
    "    return at_pattern.sub(r'', text)\n",
    "\n",
    "#Remove URLS\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "#Remove smileys\n",
    "def remove_emoji(text):   \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                            \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                            \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                            \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                            \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                            \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                            \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                            \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                            \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                            \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                            \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                            \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                            \"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "#Remove newlines.\n",
    "def remove_specialchars(text):\n",
    "    char_pattern = re.compile('[\\n]')\n",
    "    return char_pattern.sub(r'', text)\n",
    "\n",
    "#Remove only non-letters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d49c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['text'].apply(remove_ats) \\\n",
    "                                .apply(remove_urls) \\\n",
    "                                .apply(remove_tag) \\\n",
    "                                .apply(remove_emoji) \\\n",
    "                                .apply(remove_specialchars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17148b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(n=25) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a9ca06",
   "metadata": {},
   "source": [
    "### Wrapup and count again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca7e259",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset='clean_text')\n",
    "print(df.shape)\n",
    "df[\"wordcount\"] = df[\"clean_text\"].str.split().str.len()\n",
    "df['wordcount'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852a9b9e",
   "metadata": {},
   "source": [
    "### Inspect special cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092dfa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 50)\n",
    "small = df.loc[df[\"wordcount\"]<2,:].head(n=20)\n",
    "small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ac945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove remaining text not containing letters\n",
    "df = df[~df['clean_text'].str.fullmatch('^[\\s\\d]+$')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"wordcount\"]<2,:].head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f95b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Largest wordcounts seem only a few cases that I accept for now\n",
    "df.loc[df[\"wordcount\"]>65,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee44a1e",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1129417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only need tokenizer, pos tagging and lemmatization\n",
    "nlp = en_core_web_sm.load(disable=[\"parser\", \"ner\", \"textcat\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe260002",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I select for Nouns, proper Nouns and Verbs since those will the most useful in a topic detection\n",
    "in customer service setting with the end goal of automation.\n",
    "\"\"\"\n",
    "\n",
    "def preprocess(texts):\n",
    "    docs = nlp.pipe(texts, n_process=8)\n",
    "    output = []\n",
    "    for doc in docs:\n",
    "        pos_sel = \" \".join(token.lemma_ for token in doc if (token.pos_ in ['PROPN','NOUN','VERB'] and not token.is_stop))\n",
    "        output.append(pos_sel)\n",
    "    return(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_notebook.pandas()\n",
    "df['processed_text'] = preprocess(df['clean_text'])\n",
    "# test = preprocess(df.loc[:20,'clean_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb66c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51c20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/ciphix_pre_processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6dc944",
   "metadata": {},
   "source": [
    "# Topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c33e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Creating a vectorizer\n",
    "vectorizer = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(df[\"processed_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_count = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a2f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=topic_count)\n",
    "data_nmf = nmf.fit_transform(data_vectorized) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0730aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(nmf, vectorizer, topic_count):\n",
    "    res = []\n",
    "    for idx, topic in enumerate(nmf.components_):\n",
    "        descr = [(vectorizer.get_feature_names_out()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-topic_count - 1:-1]] #the final {topic_count} values\n",
    "        res.append(descr)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d89ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_classes = get_topics(nmf, vectorizer, topic_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 2, figsize=(15, 20))\n",
    "topicnr=0\n",
    "\n",
    "for ax, topic_class in zip(axs.flat, topic_classes):\n",
    "    resdf = pd.DataFrame(topic_class).rename(columns={0:'topic', 1:'score'})\n",
    "    resdf = resdf.set_index('topic').iloc[::-1]\n",
    "    ax.set_title(f'topicnr={topicnr}')\n",
    "    ax.barh(resdf.index, resdf['score'], align='center')\n",
    "    topicnr+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63f7ba9",
   "metadata": {},
   "source": [
    "# Example inference new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4082ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the new data with the fitted models\n",
    "new_doc = pd.DataFrame([\"You should also treat your skills like cattle, not pets. Yes you specialized for 10yrs in a niche that is now threatened. Be grateful that you were able to milk that skill for 10yrs, but now it might be time to move on. There’s dignity in adaptation. It’s our human superpower.\"\n",
    "                                ], dtype=str, columns=['text'])\n",
    "display(new_doc)\n",
    "\n",
    "new_doc['clean_text'] = new_doc['text'].apply(remove_ats) \\\n",
    "                                .apply(remove_urls) \\\n",
    "                                .apply(remove_tag) \\\n",
    "                                .apply(remove_emoji) \\\n",
    "                                .apply(remove_specialchars)\n",
    "\n",
    "new_doc['processed_text'] = preprocess(new_doc['clean_text'])\n",
    "\n",
    "newdata_vectorized = vectorizer.transform(new_doc[\"processed_text\"])\n",
    "newdata_nmf = nmf.transform(newdata_vectorized) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22ca53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc['processed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44e2733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top predicted topic\n",
    "predicted_topic = [each.argsort()[::-1][0] for each in newdata_nmf]\n",
    "\n",
    "# predicted_topics = print_topics(nmf, vectorizer, topic_count)\n",
    "print(predicted_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e50978",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newdata_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4448c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([word for (word,_) in topic_classes[predicted_topic[0]]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
